{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cb2679b4",
   "metadata": {},
   "source": [
    "# Model Inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "622d9045",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rhd/Documents/Raihan/Work/Data/Model-ML/spam-detection-twitter/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification, AutoConfig\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fd48af27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "import humanize\n",
    "\n",
    "\n",
    "def inspect_model_details(model_names):\n",
    "    \"\"\"\n",
    "    Inspect multiple models for a comprehensive comparison, including\n",
    "    architecture, tokenizer, and Hub metadata.\n",
    "    \"\"\"\n",
    "    print(\"üîç COMPREHENSIVE MODEL INSPECTION\")\n",
    "    print(\"=\" * 80)\n",
    "\n",
    "    # Inisialisasi API untuk mengambil data dari Hugging Face Hub\n",
    "    hf_api = HfApi()\n",
    "    results = {}\n",
    "\n",
    "    for model_name in model_names:\n",
    "        print(f\"\\nüìã Inspecting: {model_name}\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "        try:\n",
    "            # 1. Inspeksi Konfigurasi\n",
    "            config = AutoConfig.from_pretrained(model_name)\n",
    "            info = {\n",
    "                \"model_type\": config.model_type,\n",
    "                # Detail Arsitektur\n",
    "                \"hidden_size\": getattr(config, \"hidden_size\", \"N/A\"),\n",
    "                \"num_layers\": getattr(config, \"num_hidden_layers\", \"N/A\"),\n",
    "                \"num_heads\": getattr(config, \"num_attention_heads\", \"N/A\"),\n",
    "                \"num_parameters\": (\n",
    "                    humanize.intword(config.num_parameters())\n",
    "                    if hasattr(config, \"num_parameters\")\n",
    "                    and callable(config.num_parameters)\n",
    "                    else \"N/A\"\n",
    "                ),\n",
    "                # Detail Klasifikasi\n",
    "                \"num_labels\": config.num_labels,\n",
    "                \"labels\": dict(config.id2label) if hasattr(config, \"id2label\") else {},\n",
    "                \"problem_type\": getattr(config, \"problem_type\", \"Not specified\"),\n",
    "            }\n",
    "\n",
    "            print(\"   [Architecture]\")\n",
    "            print(f\"   - Model Type: {info['model_type']}\")\n",
    "            print(f\"   - Parameters: {info['num_parameters']}\")\n",
    "            print(\n",
    "                f\"   - Layers: {info['num_layers']}, Hidden Size: {info['hidden_size']}, Heads: {info['num_heads']}\"\n",
    "            )\n",
    "\n",
    "            print(\"\\n   [Classification Task]\")\n",
    "            print(f\"   - Problem Type: {info['problem_type']}\")\n",
    "            print(f\"   - Number of Labels: {info['num_labels']}\")\n",
    "            if info[\"labels\"]:\n",
    "                print(f\"   - Categories: {list(info['labels'].values())}\")\n",
    "\n",
    "            # 2. Inspeksi Tokenizer\n",
    "            try:\n",
    "                tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "                info[\"tokenizer_class\"] = tokenizer.__class__.__name__\n",
    "                info[\"vocab_size\"] = humanize.intword(tokenizer.vocab_size)\n",
    "\n",
    "                print(\"\\n   [Tokenizer]\")\n",
    "                print(f\"   - Class: {info['tokenizer_class']}\")\n",
    "                print(f\"   - Vocabulary Size: {info['vocab_size']}\")\n",
    "            except Exception as tokenizer_error:\n",
    "                print(f\"\\n   [Tokenizer]\")\n",
    "                print(f\"   - ‚ùå Error loading tokenizer: {tokenizer_error}\")\n",
    "                info[\"tokenizer_error\"] = str(tokenizer_error)\n",
    "\n",
    "            # 3. Inspeksi Metadata dari Hugging Face Hub (with better error handling)\n",
    "            try:\n",
    "                model_info_hub = hf_api.model_info(model_name)\n",
    "\n",
    "                # Safe access to attributes\n",
    "                downloads = getattr(model_info_hub, \"downloads\", 0)\n",
    "                likes = getattr(model_info_hub, \"likes\", 0)\n",
    "                last_modified = getattr(model_info_hub, \"lastModified\", None)\n",
    "\n",
    "                info[\"downloads\"] = humanize.intword(downloads) if downloads else \"N/A\"\n",
    "                info[\"likes\"] = humanize.intword(likes) if likes else \"N/A\"\n",
    "\n",
    "                # Safe date formatting\n",
    "                if last_modified:\n",
    "                    if hasattr(last_modified, \"strftime\"):\n",
    "                        info[\"last_modified\"] = last_modified.strftime(\"%Y-%m-%d\")\n",
    "                    else:\n",
    "                        info[\"last_modified\"] = str(last_modified).split(\"T\")[0]\n",
    "                else:\n",
    "                    info[\"last_modified\"] = \"N/A\"\n",
    "\n",
    "                print(\"\\n   [Hub Info]\")\n",
    "                print(f\"   - Downloads: {info['downloads']}\")\n",
    "                print(f\"   - Likes: {info['likes']}\")\n",
    "                print(f\"   - Last Modified: {info['last_modified']}\")\n",
    "\n",
    "            except Exception as hub_error:\n",
    "                print(f\"\\n   [Hub Info]\")\n",
    "                print(f\"   - ‚ùå Error accessing Hub info: {hub_error}\")\n",
    "                info[\"hub_error\"] = str(hub_error)\n",
    "                info[\"downloads\"] = \"N/A\"\n",
    "                info[\"likes\"] = \"N/A\"\n",
    "                info[\"last_modified\"] = \"N/A\"\n",
    "\n",
    "            results[model_name] = info\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"   ‚ùå Error processing {model_name}: {e}\")\n",
    "            results[model_name] = {\"error\": str(e)}\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f268b4",
   "metadata": {},
   "source": [
    "# BERT Model Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0f69aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mengunduh model (jika diperlukan)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at cahya/bert-base-indonesian-1.5G were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model siap digunakan.\n",
      "\n",
      "--- Tes untuk: 'Ibu kota negara Indonesia adalah [MASK].' ---\n",
      "Kata: jakarta         | Skor Keyakinan: 0.5408\n",
      "Kata: yogyakarta      | Skor Keyakinan: 0.0404\n",
      "Kata: pontianak       | Skor Keyakinan: 0.0294\n",
      "Kata: makassar        | Skor Keyakinan: 0.0170\n",
      "Kata: merauke         | Skor Keyakinan: 0.0158\n",
      "\n",
      "--- Tes untuk: 'Orang yang bekerja di rumah sakit biasanya adalah seorang [MASK].' ---\n",
      "Kalimat Lengkap: orang yang bekerja di rumah sakit biasanya adalah seorang dokter.\n",
      "Kalimat Lengkap: orang yang bekerja di rumah sakit biasanya adalah seorang perawat.\n",
      "Kalimat Lengkap: orang yang bekerja di rumah sakit biasanya adalah seorang bidan.\n",
      "\n",
      "--- Tes untuk: 'Setelah lelah bekerja seharian, enaknya minum [MASK] dingin.' ---\n",
      "Kalimat Lengkap: setelah lelah bekerja seharian, enaknya minum air dingin.\n",
      "Kalimat Lengkap: setelah lelah bekerja seharian, enaknya minum minuman dingin.\n",
      "Kalimat Lengkap: setelah lelah bekerja seharian, enaknya minum teh dingin.\n",
      "\n",
      "--- Tes untuk: 'Dia membeli mobil baru berwarna [MASK].' ---\n",
      "Kalimat Lengkap: dia membeli mobil baru berwarna putih.\n",
      "Kalimat Lengkap: dia membeli mobil baru berwarna hitam.\n",
      "Kalimat Lengkap: dia membeli mobil baru berwarna merah.\n"
     ]
    }
   ],
   "source": [
    "# 1. Inisialisasi pipeline \"fill-mask\"\n",
    "# Ini akan mengunduh model jika belum ada di cache\n",
    "print(\"Mengunduh model (jika diperlukan)...\")\n",
    "tebak_kata = pipeline(\"fill-mask\", model=\"cahya/bert-base-indonesian-1.5G\")\n",
    "print(\"Model siap digunakan.\")\n",
    "\n",
    "# 2. Siapkan beberapa kalimat tes\n",
    "kalimat1 = \"Ibu kota negara Indonesia adalah [MASK].\"\n",
    "kalimat2 = \"Orang yang bekerja di rumah sakit biasanya adalah seorang [MASK].\"\n",
    "kalimat3 = \"Setelah lelah bekerja seharian, enaknya minum [MASK] dingin.\"\n",
    "kalimat4 = \"Dia membeli mobil baru berwarna [MASK].\"\n",
    "\n",
    "# 3. Lakukan prediksi dan lihat hasilnya\n",
    "print(f\"\\n--- Tes untuk: '{kalimat1}' ---\")\n",
    "hasil1 = tebak_kata(kalimat1)\n",
    "for prediksi in hasil1:\n",
    "    print(\n",
    "        f\"Kata: {prediksi['token_str']:<15} | Skor Keyakinan: {prediksi['score']:.4f}\"\n",
    "    )\n",
    "\n",
    "print(f\"\\n--- Tes untuk: '{kalimat2}' ---\")\n",
    "hasil2 = tebak_kata(kalimat2, top_k=3)  # Minta 3 tebakan teratas\n",
    "for prediksi in hasil2:\n",
    "    print(f\"Kalimat Lengkap: {prediksi['sequence']}\")\n",
    "\n",
    "print(f\"\\n--- Tes untuk: '{kalimat3}' ---\")\n",
    "hasil3 = tebak_kata(kalimat3, top_k=3)\n",
    "for prediksi in hasil3:\n",
    "    print(f\"Kalimat Lengkap: {prediksi['sequence']}\")\n",
    "\n",
    "print(f\"\\n--- Tes untuk: '{kalimat4}' ---\")\n",
    "hasil4 = tebak_kata(kalimat4, top_k=3)\n",
    "for prediksi in hasil4:\n",
    "    print(f\"Kalimat Lengkap: {prediksi['sequence']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af8e42d",
   "metadata": {},
   "source": [
    "# üåç XLM-RoBERTa Model Testing\n",
    "\n",
    "Testing Facebook's XLM-RoBERTa (Cross-lingual RoBERTa) model which supports 100 languages including Indonesian. This model can be useful for:\n",
    "- Text classification tasks in multiple languages\n",
    "- Feature extraction for bot detection\n",
    "- Cross-lingual text analysis\n",
    "- Masked language modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1bea702c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ transformers already installed\n",
      "‚úÖ torch already installed\n",
      "\n",
      "üéâ All packages ready for XLM-RoBERTa testing!\n"
     ]
    }
   ],
   "source": [
    "# Install required packages for XLM-RoBERTa\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "def install_package(package):\n",
    "    try:\n",
    "        __import__(package)\n",
    "        print(f\"‚úÖ {package} already installed\")\n",
    "    except ImportError:\n",
    "        print(f\"üì¶ Installing {package}...\")\n",
    "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
    "        print(f\"‚úÖ {package} installed successfully\")\n",
    "\n",
    "# Install transformers if not already installed\n",
    "install_package(\"transformers\")\n",
    "install_package(\"torch\")\n",
    "\n",
    "print(\"\\nüéâ All packages ready for XLM-RoBERTa testing!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5963c542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Loading XLM-RoBERTa Model...\n",
      "‚ö†Ô∏è  This might take a few minutes for the first time (downloading ~2.2GB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at FacebookAI/xlm-roberta-large were not used when initializing XLMRobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ XLM-RoBERTa pipeline loaded successfully!\n",
      "\n",
      "üá∫üá∏ English test: 'I think this account is a <mask>.'\n",
      "  1. I think this account is a scam . (score: 0.6098)\n",
      "  2. I think this account is a fake . (score: 0.2386)\n",
      "  3. I think this account is a fraud . (score: 0.0285)\n",
      "  4. I think this account is a troll . (score: 0.0239)\n",
      "  5. I think this account is a hack . (score: 0.0065)\n",
      "\n",
      "üáÆüá© Indonesian test: 'Saya pikir akun ini adalah <mask>.'\n",
      "  1. Saya pikir akun ini adalah palsu . (score: 0.4199)\n",
      "  2. Saya pikir akun ini adalah penipuan . (score: 0.1155)\n",
      "  3. Saya pikir akun ini adalah scam . (score: 0.0412)\n",
      "  4. Saya pikir akun ini adalah ilegal . (score: 0.0322)\n",
      "  5. Saya pikir akun ini adalah salah . (score: 0.0210)\n",
      "\n",
      "üá®üá≥ Chinese test: 'ÊàëËÆ§‰∏∫Ëøô‰∏™Ë¥¶Êà∑ÊòØ‰∏Ä‰∏™<mask>„ÄÇ'\n",
      "  1. ÊàëËÆ§‰∏∫Ëøô‰∏™Ë¥¶Êà∑ÊòØ‰∏Ä‰∏™ÈóÆÈ¢ò „ÄÇ (score: 0.0870)\n",
      "  2. ÊàëËÆ§‰∏∫Ëøô‰∏™Ë¥¶Êà∑ÊòØ‰∏Ä‰∏™Èô∑Èò± „ÄÇ (score: 0.0749)\n",
      "  3. ÊàëËÆ§‰∏∫Ëøô‰∏™Ë¥¶Êà∑ÊòØ‰∏Ä‰∏™ÈîôËØØ „ÄÇ (score: 0.0522)\n",
      "  4. ÊàëËÆ§‰∏∫Ëøô‰∏™Ë¥¶Êà∑ÊòØ‰∏Ä‰∏™ËøùÊ≥ïË°å‰∏∫ „ÄÇ (score: 0.0352)\n",
      "  5. ÊàëËÆ§‰∏∫Ëøô‰∏™Ë¥¶Êà∑ÊòØ‰∏Ä‰∏™ÊºèÊ¥û „ÄÇ (score: 0.0241)\n",
      "\n",
      "üá∏üá¶ Arabic test: 'ÿ£ÿπÿ™ŸÇÿØ ÿ£ŸÜ Ÿáÿ∞ÿß ÿßŸÑÿ≠ÿ≥ÿßÿ® ŸáŸà <mask>.'\n",
      "  1. ÿ£ÿπÿ™ŸÇÿØ ÿ£ŸÜ Ÿáÿ∞ÿß ÿßŸÑÿ≠ÿ≥ÿßÿ® ŸáŸà ÿ≠ŸÇŸäŸÇŸä . (score: 0.1821)\n",
      "  2. ÿ£ÿπÿ™ŸÇÿØ ÿ£ŸÜ Ÿáÿ∞ÿß ÿßŸÑÿ≠ÿ≥ÿßÿ® ŸáŸà ÿßŸÑÿ≠ŸÇŸäŸÇŸä . (score: 0.0447)\n",
      "  3. ÿ£ÿπÿ™ŸÇÿØ ÿ£ŸÜ Ÿáÿ∞ÿß ÿßŸÑÿ≠ÿ≥ÿßÿ® ŸáŸà ÿ¥ÿÆÿµŸä . (score: 0.0445)\n",
      "  4. ÿ£ÿπÿ™ŸÇÿØ ÿ£ŸÜ Ÿáÿ∞ÿß ÿßŸÑÿ≠ÿ≥ÿßÿ® ŸáŸà ÿµÿ≠Ÿäÿ≠ . (score: 0.0404)\n",
      "  5. ÿ£ÿπÿ™ŸÇÿØ ÿ£ŸÜ Ÿáÿ∞ÿß ÿßŸÑÿ≠ÿ≥ÿßÿ® ŸáŸà ŸÖÿ¨ŸáŸàŸÑ . (score: 0.0384)\n",
      "\n",
      "üáØüáµ Japanese test: '„Åì„ÅÆ„Ç¢„Ç´„Ç¶„É≥„Éà„ÅØ<mask>„Å†„Å®ÊÄù„ÅÑ„Åæ„Åô„ÄÇ'\n",
      "  1. „Åì„ÅÆ„Ç¢„Ç´„Ç¶„É≥„Éà„ÅØ Anonymous „Å†„Å®ÊÄù„ÅÑ„Åæ„Åô„ÄÇ (score: 0.1109)\n",
      "  2. „Åì„ÅÆ„Ç¢„Ç´„Ç¶„É≥„Éà„ÅØ Google „Å†„Å®ÊÄù„ÅÑ„Åæ„Åô„ÄÇ (score: 0.0340)\n",
      "  3. „Åì„ÅÆ„Ç¢„Ç´„Ç¶„É≥„Éà„ÅØ fake „Å†„Å®ÊÄù„ÅÑ„Åæ„Åô„ÄÇ (score: 0.0332)\n",
      "  4. „Åì„ÅÆ„Ç¢„Ç´„Ç¶„É≥„Éà„ÅØ Twitter „Å†„Å®ÊÄù„ÅÑ„Åæ„Åô„ÄÇ (score: 0.0279)\n",
      "  5. „Åì„ÅÆ„Ç¢„Ç´„Ç¶„É≥„Éà„ÅØ Facebook „Å†„Å®ÊÄù„ÅÑ„Åæ„Åô„ÄÇ (score: 0.0239)\n",
      "\n",
      "üá∞üá∑ Korean test: 'Ïù¥ Í≥ÑÏ†ïÏùÄ <mask>ÎùºÍ≥† ÏÉùÍ∞ÅÌï©ÎãàÎã§.'\n",
      "  1. Ïù¥ Í≥ÑÏ†ïÏùÄ Anonymous ÎùºÍ≥† ÏÉùÍ∞ÅÌï©ÎãàÎã§. (score: 0.0263)\n",
      "  2. Ïù¥ Í≥ÑÏ†ïÏùÄ : ÎùºÍ≥† ÏÉùÍ∞ÅÌï©ÎãàÎã§. (score: 0.0217)\n",
      "  3. Ïù¥ Í≥ÑÏ†ïÏùÄ ÏïÑÎãà ÎùºÍ≥† ÏÉùÍ∞ÅÌï©ÎãàÎã§. (score: 0.0194)\n",
      "  4. Ïù¥ Í≥ÑÏ†ïÏùÄ ÏïÑÎãôÎãàÎã§ ÎùºÍ≥† ÏÉùÍ∞ÅÌï©ÎãàÎã§. (score: 0.0189)\n",
      "  5. Ïù¥ Í≥ÑÏ†ïÏùÄ https ÎùºÍ≥† ÏÉùÍ∞ÅÌï©ÎãàÎã§. (score: 0.0136)\n",
      "\n",
      "üá™üá∏ Spanish test: 'Creo que esta cuenta es un <mask>.'\n",
      "  1. Creo que esta cuenta es un fake . (score: 0.6725)\n",
      "  2. Creo que esta cuenta es un fraude . (score: 0.0562)\n",
      "  3. Creo que esta cuenta es un spam . (score: 0.0372)\n",
      "  4. Creo que esta cuenta es un troll . (score: 0.0370)\n",
      "  5. Creo que esta cuenta es un falso . (score: 0.0167)\n"
     ]
    }
   ],
   "source": [
    "# Basic XLM-RoBERTa Model Testing\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForMaskedLM\n",
    "import torch\n",
    "\n",
    "print(\"üöÄ Loading XLM-RoBERTa Model...\")\n",
    "print(\"‚ö†Ô∏è  This might take a few minutes for the first time (downloading ~2.2GB)\")\n",
    "\n",
    "# Method 1: Using pipeline (High-level API)\n",
    "try:\n",
    "    # Load the fill-mask pipeline\n",
    "    xlm_roberta_pipe = pipeline(\n",
    "        \"fill-mask\",\n",
    "        model=\"FacebookAI/xlm-roberta-large\",\n",
    "        tokenizer=\"FacebookAI/xlm-roberta-large\"\n",
    "    )\n",
    "    print(\"‚úÖ XLM-RoBERTa pipeline loaded successfully!\")\n",
    "\n",
    "    # Test with English\n",
    "    english_test = \"I think this account is a <mask>.\"\n",
    "    print(f\"\\nüá∫üá∏ English test: '{english_test}'\")\n",
    "    english_results = xlm_roberta_pipe(english_test, top_k=5)\n",
    "\n",
    "    for i, result in enumerate(english_results, 1):\n",
    "        print(f\"  {i}. {result['sequence']} (score: {result['score']:.4f})\")\n",
    "\n",
    "    # Test with Indonesian\n",
    "    indonesian_test = \"Saya pikir akun ini adalah <mask>.\"\n",
    "    print(f\"\\nüáÆüá© Indonesian test: '{indonesian_test}'\")\n",
    "    indonesian_results = xlm_roberta_pipe(indonesian_test, top_k=5)\n",
    "\n",
    "    for i, result in enumerate(indonesian_results, 1):\n",
    "        print(f\"  {i}. {result['sequence']} (score: {result['score']:.4f})\")\n",
    "\n",
    "    # Test with china\n",
    "    chinese_test = \"ÊàëËÆ§‰∏∫Ëøô‰∏™Ë¥¶Êà∑ÊòØ‰∏Ä‰∏™<mask>„ÄÇ\"\n",
    "    print(f\"\\nüá®üá≥ Chinese test: '{chinese_test}'\")\n",
    "    chinese_results = xlm_roberta_pipe(chinese_test, top_k=5)\n",
    "\n",
    "    for i, result in enumerate(chinese_results, 1):\n",
    "        print(f\"  {i}. {result['sequence']} (score: {result['score']:.4f})\")\n",
    "\n",
    "    # Test with arabic\n",
    "    arabic_test = \"ÿ£ÿπÿ™ŸÇÿØ ÿ£ŸÜ Ÿáÿ∞ÿß ÿßŸÑÿ≠ÿ≥ÿßÿ® ŸáŸà <mask>.\"\n",
    "    print(f\"\\nüá∏üá¶ Arabic test: '{arabic_test}'\")\n",
    "    arabic_results = xlm_roberta_pipe(arabic_test, top_k=5)\n",
    "\n",
    "    for i, result in enumerate(arabic_results, 1):\n",
    "        print(f\"  {i}. {result['sequence']} (score: {result['score']:.4f})\")\n",
    "\n",
    "    # Test with japanese\n",
    "    japanese_test = \"„Åì„ÅÆ„Ç¢„Ç´„Ç¶„É≥„Éà„ÅØ<mask>„Å†„Å®ÊÄù„ÅÑ„Åæ„Åô„ÄÇ\"\n",
    "    print(f\"\\nüáØüáµ Japanese test: '{japanese_test}'\")\n",
    "    japanese_results = xlm_roberta_pipe(japanese_test, top_k=5)\n",
    "\n",
    "    for i, result in enumerate(japanese_results, 1):\n",
    "        print(f\"  {i}. {result['sequence']} (score: {result['score']:.4f})\")\n",
    "\n",
    "    # Test with korean\n",
    "    korean_test = \"Ïù¥ Í≥ÑÏ†ïÏùÄ <mask>ÎùºÍ≥† ÏÉùÍ∞ÅÌï©ÎãàÎã§.\"\n",
    "    print(f\"\\nüá∞üá∑ Korean test: '{korean_test}'\")\n",
    "    korean_results = xlm_roberta_pipe(korean_test, top_k=5)\n",
    "\n",
    "    for i, result in enumerate(korean_results, 1):\n",
    "        print(f\"  {i}. {result['sequence']} (score: {result['score']:.4f})\")\n",
    "\n",
    "    # Test with spanish\n",
    "    spanish_test = \"Creo que esta cuenta es un <mask>.\"\n",
    "    print(f\"\\nüá™üá∏ Spanish test: '{spanish_test}'\")\n",
    "    spanish_results = xlm_roberta_pipe(spanish_test, top_k=5)\n",
    "\n",
    "    for i, result in enumerate(spanish_results, 1):\n",
    "        print(f\"  {i}. {result['sequence']} (score: {result['score']:.4f})\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading pipeline: {e}\")\n",
    "    xlm_roberta_pipe = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8760aaf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Loading XLM-RoBERTa with direct model access...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at FacebookAI/xlm-roberta-large were not used when initializing XLMRobertaForMaskedLM: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ XLM-RoBERTa tokenizer and model loaded successfully!\n",
      "üìä Model config:\n",
      "   - Vocab size: 250,002\n",
      "   - Max length: 512\n",
      "   - Model parameters: ~560,142,482\n",
      "\n",
      "üîç Testing: 'Replace me by any text you'd like.'\n",
      "   üìù Tokenized: ['<s>', '‚ñÅRe', 'place', '‚ñÅme', '‚ñÅby', '‚ñÅany', '‚ñÅtext', '‚ñÅyou', \"'\", 'd', '‚ñÅlike', '.', '</s>']\n",
      "   üß† Hidden states shape: torch.Size([1, 13, 1024])\n",
      "   üìä Feature vector size per token: 1024\n",
      "\n",
      "üîç Testing: 'Ganti saya dengan teks apa pun yang Anda suka.'\n",
      "   üìù Tokenized: ['<s>', '‚ñÅGan', 'ti', '‚ñÅsaya', '‚ñÅdengan', '‚ñÅteks', '‚ñÅapa', '‚ñÅpun', '‚ñÅyang', '‚ñÅAnda', '‚ñÅsuka', '.', '</s>']\n",
      "   üß† Hidden states shape: torch.Size([1, 13, 1024])\n",
      "   üìä Feature vector size per token: 1024\n",
      "\n",
      "üîç Testing: '„Åì„ÅÆ„ÉÜ„Ç≠„Çπ„Éà„ÇíÂ•Ω„Åç„Å™„ÇÇ„ÅÆ„Å´ÁΩÆ„ÅçÊèõ„Åà„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ'\n",
      "   üìù Tokenized: ['<s>', '‚ñÅ„Åì„ÅÆ', '„ÉÜ„Ç≠„Çπ„Éà', '„Çí', 'Â•Ω„Åç„Å™', '„ÇÇ„ÅÆ', '„Å´', 'ÁΩÆ„Åç', 'Êèõ„Åà', '„Å¶„Åè„Å†„Åï„ÅÑ', '„ÄÇ', '</s>']\n",
      "   üß† Hidden states shape: torch.Size([1, 12, 1024])\n",
      "   üìä Feature vector size per token: 1024\n"
     ]
    }
   ],
   "source": [
    "# Method 2: Direct model loading (Lower-level API)\n",
    "print(\"üîß Loading XLM-RoBERTa with direct model access...\")\n",
    "\n",
    "try:\n",
    "    # Load tokenizer and model directly\n",
    "    xlm_tokenizer = AutoTokenizer.from_pretrained(\"FacebookAI/xlm-roberta-large\")\n",
    "    xlm_model = AutoModelForMaskedLM.from_pretrained(\"FacebookAI/xlm-roberta-large\")\n",
    "\n",
    "    print(\"‚úÖ XLM-RoBERTa tokenizer and model loaded successfully!\")\n",
    "    print(f\"üìä Model config:\")\n",
    "    print(f\"   - Vocab size: {xlm_tokenizer.vocab_size:,}\")\n",
    "    print(f\"   - Max length: {xlm_tokenizer.model_max_length}\")\n",
    "    print(f\"   - Model parameters: ~{sum(p.numel() for p in xlm_model.parameters()):,}\")\n",
    "\n",
    "    # Test direct usage\n",
    "    def test_xlm_roberta_direct(text):\n",
    "        print(f\"\\nüîç Testing: '{text}'\")\n",
    "\n",
    "        # Tokenize\n",
    "        inputs = xlm_tokenizer(text, return_tensors='pt')\n",
    "        print(f\"   üìù Tokenized: {xlm_tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])}\")\n",
    "\n",
    "        # Get embeddings/features\n",
    "        with torch.no_grad():\n",
    "            outputs = xlm_model(**inputs, output_hidden_states=True)\n",
    "            # Get the hidden states (features)\n",
    "            hidden_states = outputs.hidden_states[-1]  # Last layer hidden states\n",
    "\n",
    "        print(f\"   üß† Hidden states shape: {hidden_states.shape}\")\n",
    "        print(f\"   üìä Feature vector size per token: {hidden_states.shape[-1]}\")\n",
    "\n",
    "        return hidden_states\n",
    "\n",
    "    # Test with different languages\n",
    "    test_texts = [\n",
    "        \"Replace me by any text you'd like.\",  # English\n",
    "        \"Ganti saya dengan teks apa pun yang Anda suka.\",  # Indonesian\n",
    "        \"„Åì„ÅÆ„ÉÜ„Ç≠„Çπ„Éà„ÇíÂ•Ω„Åç„Å™„ÇÇ„ÅÆ„Å´ÁΩÆ„ÅçÊèõ„Åà„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\"  # Japanese\n",
    "    ]\n",
    "\n",
    "    for text in test_texts:\n",
    "        features = test_xlm_roberta_direct(text)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading model directly: {e}\")\n",
    "    xlm_tokenizer = None\n",
    "    xlm_model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5593608d",
   "metadata": {},
   "source": [
    "# Spam Analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6b9d1ac9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç COMPREHENSIVE MODEL INSPECTION\n",
      "================================================================================\n",
      "\n",
      "üìã Inspecting: plipustel/IndoBERT-spam-detector\n",
      "------------------------------------------------------------\n",
      "   [Architecture]\n",
      "   - Model Type: bert\n",
      "   - Parameters: N/A\n",
      "   - Layers: 12, Hidden Size: 768, Heads: 12\n",
      "\n",
      "   [Classification Task]\n",
      "   - Problem Type: single_label_classification\n",
      "   - Number of Labels: 5\n",
      "   - Categories: ['LABEL_0', 'LABEL_1', 'LABEL_2', 'LABEL_3', 'LABEL_4']\n",
      "\n",
      "   [Tokenizer]\n",
      "   - Class: BertTokenizerFast\n",
      "   - Vocabulary Size: 30.5 thousand\n",
      "\n",
      "   [Hub Info]\n",
      "   - Downloads: 82\n",
      "   - Likes: N/A\n",
      "   - Last Modified: 2025-06-07\n",
      "\n",
      "üìã Inspecting: iqbalpurba26/indobert-ham-spam-detection\n",
      "------------------------------------------------------------\n",
      "   [Architecture]\n",
      "   - Model Type: bert\n",
      "   - Parameters: N/A\n",
      "   - Layers: 12, Hidden Size: 768, Heads: 12\n",
      "\n",
      "   [Classification Task]\n",
      "   - Problem Type: None\n",
      "   - Number of Labels: 2\n",
      "   - Categories: ['LABEL_0', 'LABEL_1']\n",
      "\n",
      "   [Tokenizer]\n",
      "   - Class: BertTokenizerFast\n",
      "   - Vocabulary Size: 30.5 thousand\n",
      "\n",
      "   [Hub Info]\n",
      "   - Downloads: 12\n",
      "   - Likes: N/A\n",
      "   - Last Modified: 2024-12-18\n",
      "\n",
      "üìã Inspecting: kasyfilalbar/indo-spam-chatbot\n",
      "------------------------------------------------------------\n",
      "   [Architecture]\n",
      "   - Model Type: gemma2\n",
      "   - Parameters: N/A\n",
      "   - Layers: 26, Hidden Size: 2304, Heads: 8\n",
      "\n",
      "   [Classification Task]\n",
      "   - Problem Type: single_label_classification\n",
      "   - Number of Labels: 2\n",
      "   - Categories: ['LABEL_0', 'LABEL_1']\n",
      "\n",
      "   [Tokenizer]\n",
      "   - Class: GemmaTokenizerFast\n",
      "   - Vocabulary Size: 256.0 thousand\n",
      "\n",
      "   [Hub Info]\n",
      "   - Downloads: 37\n",
      "   - Likes: 1\n",
      "   - Last Modified: 2024-11-18\n"
     ]
    }
   ],
   "source": [
    "models_to_compare = [\n",
    "    \"plipustel/IndoBERT-spam-detector\",\n",
    "    \"iqbalpurba26/indobert-ham-spam-detection\",\n",
    "    \"kasyfilalbar/indo-spam-chatbot\",\n",
    "]\n",
    "\n",
    "detailed_results = inspect_model_details(models_to_compare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "273f60ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Testing Indo Spam Chatbot Analyzer...\n",
      "üîÑ Loading Indo Spam Chatbot model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:00<00:00, 38.66it/s]\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Indo Spam Chatbot model loaded successfully!\n",
      "üîß Using device: cpu\n",
      "üè∑Ô∏è Labels: {0: 'Non-spam', 1: 'Spam'}\n",
      "\n",
      "üß™ Testing individual spam predictions:\n",
      "----------------------------------------------------------------------\n",
      "Message: 'adsfwcasdfad'\n",
      "   Expected: Spam (gibberish)\n",
      "   üö® Result: Spam (0.998) - high\n",
      "   üìä Probabilities: Non-spam: 0.002, Spam: 0.998\n",
      "\n",
      "Message: 'kak bisa depo di link ini: http://dewa.site/dewa/dewi'\n",
      "   Expected: Spam (suspicious link)\n",
      "   üö® Result: Spam (0.996) - high\n",
      "   üìä Probabilities: Non-spam: 0.004, Spam: 0.996\n",
      "\n",
      "Message: 'p'\n",
      "   Expected: Spam (single character)\n",
      "   üö® Result: Spam (0.999) - high\n",
      "   üìä Probabilities: Non-spam: 0.001, Spam: 0.999\n",
      "\n",
      "Message: '1234'\n",
      "   Expected: Spam (numbers only)\n",
      "   üö® Result: Spam (0.998) - high\n",
      "   üìä Probabilities: Non-spam: 0.002, Spam: 0.998\n",
      "\n",
      "Message: 'Halo, bagaimana kabar Anda hari ini?'\n",
      "   Expected: Non-spam (normal conversation)\n",
      "   ‚úÖ Result: Non-spam (0.985) - high\n",
      "   üìä Probabilities: Non-spam: 0.985, Spam: 0.015\n",
      "\n",
      "Message: 'Terima kasih atas informasinya'\n",
      "   Expected: Non-spam (polite message)\n",
      "   ‚úÖ Result: Non-spam (1.000) - high\n",
      "   üìä Probabilities: Non-spam: 1.000, Spam: 0.000\n",
      "\n",
      "Message: 'KLIK DISINI MENANG 100 JUTA!!!'\n",
      "   Expected: Spam (promotional spam)\n",
      "   üö® Result: Spam (0.785) - medium\n",
      "   üìä Probabilities: Non-spam: 0.215, Spam: 0.785\n",
      "\n",
      "Message: 'Selamat pagi, apakah bisa bantu saya?'\n",
      "   Expected: Non-spam (normal inquiry)\n",
      "   ‚úÖ Result: Non-spam (0.969) - high\n",
      "   üìä Probabilities: Non-spam: 0.969, Spam: 0.031\n",
      "\n",
      "Message: 'wwwwwwwwwwwwwww'\n",
      "   Expected: Spam (repeated characters)\n",
      "   üö® Result: Spam (1.000) - high\n",
      "   üìä Probabilities: Non-spam: 0.000, Spam: 1.000\n",
      "\n",
      "Message: 'Mohon maaf mengganggu waktu Anda'\n",
      "   Expected: Non-spam (polite message)\n",
      "   ‚úÖ Result: Non-spam (0.997) - high\n",
      "   üìä Probabilities: Non-spam: 0.997, Spam: 0.003\n",
      "\n",
      "‚úÖ Indo Spam Chatbot Analyzer test completed!\n"
     ]
    }
   ],
   "source": [
    "class IndoSpamChatbotAnalyzer:\n",
    "    def __init__(self, model_name=\"kasyfilalbar/indo-spam-chatbot\"):\n",
    "        \"\"\"Initialize Indo Spam Chatbot analyzer\"\"\"\n",
    "        print(\"üîÑ Loading Indo Spam Chatbot model...\")\n",
    "\n",
    "        try:\n",
    "            # Load tokenizer and model\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "            self.model = AutoModelForSequenceClassification.from_pretrained(\n",
    "                model_name, device_map=\"auto\" if torch.cuda.is_available() else None\n",
    "            )\n",
    "\n",
    "            # Set device\n",
    "            self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "            if not torch.cuda.is_available():\n",
    "                self.model = self.model.to(self.device)\n",
    "\n",
    "            # Labels berdasarkan dokumentasi\n",
    "            self.labels = {0: \"Non-spam\", 1: \"Spam\"}\n",
    "\n",
    "            print(\"‚úÖ Indo Spam Chatbot model loaded successfully!\")\n",
    "            print(f\"üîß Using device: {self.device}\")\n",
    "            print(f\"üè∑Ô∏è Labels: {self.labels}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading model: {e}\")\n",
    "            raise e\n",
    "\n",
    "    def predict_single(self, text):\n",
    "        \"\"\"Predict spam for a single text\"\"\"\n",
    "        if pd.isna(text) or text is None or text == \"\" or text == \"No Comment\":\n",
    "            return {\"label\": \"Non-spam\", \"score\": 0.0, \"confidence\": \"low\"}\n",
    "\n",
    "        try:\n",
    "            # Tokenize\n",
    "            encoded_input = self.tokenizer(\n",
    "                str(text), padding=True, truncation=True, return_tensors=\"pt\"\n",
    "            )\n",
    "\n",
    "            # Move to device\n",
    "            encoded_input = {k: v.to(self.device) for k, v in encoded_input.items()}\n",
    "\n",
    "            # Predict\n",
    "            with torch.no_grad():\n",
    "                model_output = self.model(**encoded_input)\n",
    "                logits = model_output.logits\n",
    "\n",
    "                # Get probabilities\n",
    "                probabilities = torch.softmax(logits, dim=1)\n",
    "                predicted_label = torch.argmax(logits, dim=1).item()\n",
    "                confidence_score = probabilities[0][predicted_label].item()\n",
    "\n",
    "            # Map to label\n",
    "            label = self.labels[predicted_label]\n",
    "\n",
    "            # Determine confidence level\n",
    "            if confidence_score >= 0.8:\n",
    "                confidence = \"high\"\n",
    "            elif confidence_score >= 0.6:\n",
    "                confidence = \"medium\"\n",
    "            else:\n",
    "                confidence = \"low\"\n",
    "\n",
    "            return {\n",
    "                \"label\": label,\n",
    "                \"score\": confidence_score,\n",
    "                \"confidence\": confidence,\n",
    "                \"probabilities\": {\n",
    "                    \"Non-spam\": probabilities[0][0].item(),\n",
    "                    \"Spam\": probabilities[0][1].item(),\n",
    "                },\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error predicting spam: {e}\")\n",
    "            return {\"label\": \"Non-spam\", \"score\": 0.0, \"confidence\": \"error\"}\n",
    "\n",
    "    def predict_batch(self, texts, batch_size=32):\n",
    "        \"\"\"Predict spam for multiple texts efficiently\"\"\"\n",
    "        results = []\n",
    "\n",
    "        # Convert to list if pandas Series\n",
    "        if hasattr(texts, \"tolist\"):\n",
    "            texts = texts.tolist()\n",
    "\n",
    "        print(f\"üîÑ Processing {len(texts)} texts for spam detection...\")\n",
    "        print(f\"üìä Batch size: {batch_size}\")\n",
    "\n",
    "        # Process in batches\n",
    "        for i in tqdm(range(0, len(texts), batch_size), desc=\"Analyzing spam\"):\n",
    "            batch = texts[i : i + batch_size]\n",
    "\n",
    "            # Clean batch texts\n",
    "            clean_batch = []\n",
    "            for text in batch:\n",
    "                if pd.isna(text) or text is None or text == \"\" or text == \"No Comment\":\n",
    "                    clean_batch.append(\"No Comment\")\n",
    "                else:\n",
    "                    clean_batch.append(str(text))\n",
    "\n",
    "            try:\n",
    "                # Tokenize batch\n",
    "                encoded_inputs = self.tokenizer(\n",
    "                    clean_batch, padding=True, truncation=True, return_tensors=\"pt\"\n",
    "                )\n",
    "\n",
    "                # Move to device\n",
    "                encoded_inputs = {\n",
    "                    k: v.to(self.device) for k, v in encoded_inputs.items()\n",
    "                }\n",
    "\n",
    "                # Predict batch\n",
    "                with torch.no_grad():\n",
    "                    model_outputs = self.model(**encoded_inputs)\n",
    "                    logits = model_outputs.logits\n",
    "                    probabilities = torch.softmax(logits, dim=1)\n",
    "                    predicted_labels = torch.argmax(logits, dim=1)\n",
    "\n",
    "                # Process results\n",
    "                for j in range(len(batch)):\n",
    "                    if clean_batch[j] == \"No Comment\":\n",
    "                        results.append(\n",
    "                            {\"label\": \"Non-spam\", \"score\": 0.0, \"confidence\": \"low\"}\n",
    "                        )\n",
    "                    else:\n",
    "                        label_idx = predicted_labels[j].item()\n",
    "                        confidence_score = probabilities[j][label_idx].item()\n",
    "                        label = self.labels[label_idx]\n",
    "\n",
    "                        # Determine confidence level\n",
    "                        if confidence_score >= 0.8:\n",
    "                            confidence = \"high\"\n",
    "                        elif confidence_score >= 0.6:\n",
    "                            confidence = \"medium\"\n",
    "                        else:\n",
    "                            confidence = \"low\"\n",
    "\n",
    "                        results.append(\n",
    "                            {\n",
    "                                \"label\": label,\n",
    "                                \"score\": confidence_score,\n",
    "                                \"confidence\": confidence,\n",
    "                            }\n",
    "                        )\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error in spam batch {i//batch_size + 1}: {e}\")\n",
    "                # Add default predictions for failed batch\n",
    "                for _ in range(len(batch)):\n",
    "                    results.append(\n",
    "                        {\"label\": \"Non-spam\", \"score\": 0.0, \"confidence\": \"error\"}\n",
    "                    )\n",
    "\n",
    "        return results\n",
    "\n",
    "    def analyze_results(self, results):\n",
    "        \"\"\"Analyze and display spam detection results\"\"\"\n",
    "        total = len(results)\n",
    "\n",
    "        print(f\"\\nüìä SPAM DETECTION SUMMARY\")\n",
    "        print(\"=\" * 50)\n",
    "        print(f\"üìù Total texts: {total}\")\n",
    "\n",
    "        # Spam distribution\n",
    "        spam_counts = {\"Spam\": 0, \"Non-spam\": 0}\n",
    "        for result in results:\n",
    "            label = result[\"label\"]\n",
    "            spam_counts[label] = spam_counts.get(label, 0) + 1\n",
    "\n",
    "        print(f\"\\nüö® Spam Distribution:\")\n",
    "        for category, count in spam_counts.items():\n",
    "            percentage = (count / total) * 100\n",
    "            emoji = \"üö®\" if category == \"Spam\" else \"‚úÖ\"\n",
    "            print(f\"   {emoji} {category}: {count} ({percentage:.1f}%)\")\n",
    "\n",
    "        # Confidence distribution\n",
    "        conf_counts = {\"high\": 0, \"medium\": 0, \"low\": 0, \"error\": 0}\n",
    "        for result in results:\n",
    "            conf_counts[result[\"confidence\"]] += 1\n",
    "\n",
    "        print(f\"\\nüéØ Confidence Distribution:\")\n",
    "        for conf, count in conf_counts.items():\n",
    "            percentage = (count / total) * 100\n",
    "            print(f\"   {conf.capitalize()}: {count} ({percentage:.1f}%)\")\n",
    "\n",
    "        return {\n",
    "            \"total\": total,\n",
    "            \"spam_distribution\": spam_counts,\n",
    "            \"confidence_distribution\": conf_counts,\n",
    "        }\n",
    "\n",
    "\n",
    "# Test Indo Spam Chatbot Analyzer\n",
    "print(\"üöÄ Testing Indo Spam Chatbot Analyzer...\")\n",
    "\n",
    "try:\n",
    "    # Initialize analyzer\n",
    "    spam_analyzer = IndoSpamChatbotAnalyzer()\n",
    "\n",
    "    # Test dengan contoh dari dokumentasi + tambahan\n",
    "    test_messages = [\n",
    "        (\"adsfwcasdfad\", \"Expected: Spam (gibberish)\"),\n",
    "        (\n",
    "            \"kak bisa depo di link ini: http://dewa.site/dewa/dewi\",\n",
    "            \"Expected: Spam (suspicious link)\",\n",
    "        ),\n",
    "        (\"p\", \"Expected: Spam (single character)\"),\n",
    "        (\"1234\", \"Expected: Spam (numbers only)\"),\n",
    "        (\n",
    "            \"Halo, bagaimana kabar Anda hari ini?\",\n",
    "            \"Expected: Non-spam (normal conversation)\",\n",
    "        ),\n",
    "        (\"Terima kasih atas informasinya\", \"Expected: Non-spam (polite message)\"),\n",
    "        (\"KLIK DISINI MENANG 100 JUTA!!!\", \"Expected: Spam (promotional spam)\"),\n",
    "        (\n",
    "            \"Selamat pagi, apakah bisa bantu saya?\",\n",
    "            \"Expected: Non-spam (normal inquiry)\",\n",
    "        ),\n",
    "        (\"wwwwwwwwwwwwwww\", \"Expected: Spam (repeated characters)\"),\n",
    "        (\"Mohon maaf mengganggu waktu Anda\", \"Expected: Non-spam (polite message)\"),\n",
    "    ]\n",
    "\n",
    "    print(f\"\\nüß™ Testing individual spam predictions:\")\n",
    "    print(\"-\" * 70)\n",
    "\n",
    "    for message, expected in test_messages:\n",
    "        result = spam_analyzer.predict_single(message)\n",
    "        spam_emoji = \"üö®\" if result[\"label\"] == \"Spam\" else \"‚úÖ\"\n",
    "\n",
    "        print(f\"Message: '{message}'\")\n",
    "        print(f\"   {expected}\")\n",
    "        print(\n",
    "            f\"   {spam_emoji} Result: {result['label']} ({result['score']:.3f}) - {result['confidence']}\"\n",
    "        )\n",
    "        if \"probabilities\" in result:\n",
    "            print(\n",
    "                f\"   üìä Probabilities: Non-spam: {result['probabilities']['Non-spam']:.3f}, Spam: {result['probabilities']['Spam']:.3f}\"\n",
    "            )\n",
    "        print()\n",
    "\n",
    "    print(\"‚úÖ Indo Spam Chatbot Analyzer test completed!\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error in Indo Spam Chatbot Analyzer: {e}\")\n",
    "    import traceback\n",
    "\n",
    "    traceback.print_exc()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
